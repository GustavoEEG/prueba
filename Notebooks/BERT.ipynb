{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27d091e",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d75d1",
   "metadata": {},
   "source": [
    "## Training BERT for Binary Text Classification using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd91221",
   "metadata": {},
   "source": [
    "Configuring your Python TensorFlow environment is quite straightforward:\n",
    "\n",
    "* Clone the BERT GitHub repository onto your computer. In your terminal, enter the following command: git clone https://github.com/google-research/bert.git\n",
    "* Obtain the pre-trained BERT model files from the official BERT GitHub page. These files contain the weights, hyperparameters, and other essential information BERT acquired during pre-training. Save these files in the directory where you cloned the GitHub repository and then extract them. The following links provide access to different files:\n",
    "    * BERT-Large, Uncased (Whole Word Masking): 24-layer, 1024-hidden, 16-heads, 340M parameters: https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "    * BERT-Large, Cased (Whole Word Masking): 24-layer, 1024-hidden, 16-heads, 340M parameters: https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\n",
    "    * BERT-Base, Multilingual Cased: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters: https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "    * BERT-Base, Multilingual (Not recommended, use Multilingual Cased instead): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters: https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\n",
    "\n",
    "More can be found on GitHub: https://github.com/google-research/bert.git\n",
    "\n",
    "We observe that there are files designated as \"cased\" and \"uncased,\" indicating whether letter casing is deemed beneficial for the task in question. In our example, we opted to download the BERT-Base-Cased model.\n",
    "\n",
    "To utilize BERT, it is necessary to transform our data into the format BERT anticipates. BERT requires data to be in a TSV file with a specific structure with four columns: \n",
    "    * Column 0: A unique identifier for each row\n",
    "    * Column 1: An integer label for the row (class labels: 0, 1, 2, 3, etc.)\n",
    "    * Column 2: A consistent letter for all rows, included solely because BERT expects it, though it serves no purpose.\n",
    "    * Column 3: The text samples we aim to classify\n",
    "\n",
    "In the subsequent analysis, we will interact with the Yelp Reviews Polarity dataset. By leveraging the pandas library, we will import and meticulously analyze this information. The dataset comprises user-generated assessments and ratings for a wide variety of businesses, primarily centered on restaurants and local services, as featured on Yelp's platform. This aggregation of data provides crucial insights into consumer preferences, experiences, and viewpoints, thus facilitating businesses and researchers in deciphering customer behavior and enhancing their offerings. The dataset can be accessed at https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews. Designed for binary sentiment classification, the Yelp Reviews Polarity dataset includes 560,000 highly polarized Yelp reviews for training and an additional 38,000 for testing. This dataset originates from Yelp reviews and constitutes a portion of the Yelp Dataset Challenge 2015 data. For additional details, please visit http://www.yelp.com/dataset.\n",
    "\n",
    "As delineated above, it is necessary to create a folder within the directory where BERT was cloned, which will house three distinct files: train.tsv, dev.tsv, and test.tsv (where TSV denotes tab-separated values). Both train.tsv and dev.tsv should encompass all four columns, while test.tsv ought to contain only two columns, specifically the row ID and the text designated for classification.\n",
    "\n",
    "Additionally, we should create a folder named \"data\" within the \"bert\" directory to store the .tsv files and another folder called \"bert_output\" where the fine-tuned model will be saved. The pre-trained BERT model should be stored in the \"bert\" directory as well.\n",
    "\n",
    "Let’s prepare our data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24b4c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Unfortunately, the frustration of being Dr. Go...\n",
       "1      2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2      1  I don't know what Dr. Goldberg was like before...\n",
       "3      1  I'm writing this review to give you a heads up...\n",
       "4      2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use pandas read_csv function to load the Yelp Reviews Polarity dataset into a DataFrame\n",
    "# The dataset is stored in the file 'train.csv' and 'test.csv' with comma-separated values\n",
    "# Assign column names 'label' and 'text' to the respective columns in the DataFrames\n",
    "df_bert_train = pd.read_csv('../data/datasets/yelp_review_polarity_csv/train.csv', names=['label', 'text'])\n",
    "df_bert_test = pd.read_csv('../data/datasets/yelp_review_polarity_csv/test.csv', names=['label', 'text'])\n",
    "\n",
    "# Display the first five rows of the training DataFrame to verify the data import\n",
    "df_bert_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a06183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Unfortunately, the frustration of being Dr. Go...\n",
       "1      1  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2      0  I don't know what Dr. Goldberg was like before...\n",
       "3      0  I'm writing this review to give you a heads up...\n",
       "4      1  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a LabelEncoder object\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Use the LabelEncoder object to fit and transform the 'label' column in the DataFrame\n",
    "# This converts the original labels into integer-encoded labels\n",
    "df_bert_train['label'] = labelencoder.fit_transform(df_bert_train['label'])\n",
    "df_bert_test['label'] = labelencoder.fit_transform(df_bert_test['label'])\n",
    "\n",
    "# Show the first five rows of the DataFrame, displaying the transformed 'label' column\n",
    "df_bert_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a529997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  Unfortunately, the frustration of being Dr. Go...\n",
       "1   1      1     a  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2   2      0     a  I don't know what Dr. Goldberg was like before...\n",
       "3   3      0     a  I'm writing this review to give you a heads up...\n",
       "4   4      1     a  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a new DataFrame for training in compliance with BERT's specifications\n",
    "df_bert_train = pd.DataFrame({\n",
    "    'id': range(len(df_bert_train)),                 # Create an 'id' column with a sequence of integers from 0 to the length of df_bert_train\n",
    "    'label': df_bert_train['label'],                 # Incorporate the integer-encoded 'label' column from the existing df_bert_train DataFrame\n",
    "    'alpha': ['a'] * df_bert_train.shape[0],         # Generate a dummy 'alpha' column with the same letter 'a' for all rows\n",
    "    'text': df_bert_train['text'].replace(r'\\n', ' ', regex=True) # Introduce a 'text' column containing the text from the 'text' column, substituting newline characters with spaces\n",
    "})\n",
    "\n",
    "# Showcase the first five rows of the newly established DataFrame\n",
    "df_bert_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad146ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      1     a  Contrary to other reviews, I have zero complai...\n",
       "1   1      0     a  Last summer I had an appointment to get new ti...\n",
       "2   2      1     a  Friendly staff, same starbucks fair you get an...\n",
       "3   3      0     a  The food is good. Unfortunately the service is...\n",
       "4   4      1     a  Even when we didn't have a car Filene's Baseme..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a new DataFrame for testing in compliance with BERT's specifications\n",
    "df_bert_test = pd.DataFrame({\n",
    "    'id': range(len(df_bert_test)),                 # Create an 'id' column with a sequence of integers from 0 to the length of df_bert_test\n",
    "    'label': df_bert_test['label'],                 # Incorporate the integer-encoded 'label' column from the existing df_bert_test DataFrame\n",
    "    'alpha': ['a'] * df_bert_test.shape[0],         # Generate a dummy 'alpha' column with the same letter 'a' for all rows\n",
    "    'text': df_bert_test['text'].replace(r'\\n', ' ', regex=True) # Introduce a 'text' column containing the text from the 'text' column, substituting newline characters with spaces\n",
    "})\n",
    "\n",
    "# Showcase the first five rows of the newly established DataFrame\n",
    "df_bert_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d04cbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            id  label alpha                                               text\n",
       " 417837  417837      1     a  Really good food and great service. I wouldn't...\n",
       " 383622  383622      1     a  Ah, happiness is a new buffet restaurant in to...\n",
       " 200378  200378      1     a  Sure there's lots of stuff to do inside this e...\n",
       " 216246  216246      0     a  Sylvia, what happened?\\nWe normally do take ou...\n",
       " 439684  439684      0     a  First of all, after being told \\\"Oh my Gosh, I...,\n",
       "    id  label alpha                                               text\n",
       " 0   0      1     a  Contrary to other reviews, I have zero complai...\n",
       " 1   1      0     a  Last summer I had an appointment to get new ti...\n",
       " 2   2      1     a  Friendly staff, same starbucks fair you get an...\n",
       " 3   3      0     a  The food is good. Unfortunately the service is...\n",
       " 4   4      1     a  Even when we didn't have a car Filene's Baseme...,\n",
       "             id  label alpha                                               text\n",
       " 90605    90605      0     a  We are here for spring training. The Potsticke...\n",
       " 16654    16654      1     a  A really nice place, attentive service, and no...\n",
       " 507856  507856      1     a  The location is superb of this frozen yogurt p...\n",
       " 103976  103976      0     a  I have to give a star but if I could, I would ...\n",
       " 181086  181086      1     a  Normally stop in here after eating at Aloha Sp...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the train set further into train and dev (development/validation) sets\n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert_train, test_size=0.01)\n",
    "\n",
    "# Display the first five rows of each set (train, test, and dev) by calling the head() method\n",
    "df_bert_train.head(), df_bert_test.head(), df_bert_dev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc443d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to .tsv format, as required by BERT\n",
    "df_bert_train.to_csv('../../bert/data/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv('../../bert/data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('../../bert/data/test.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6d877",
   "metadata": {},
   "source": [
    "With our terminal, we can go to the “bert” folder and type the following command line:\n",
    "\n",
    "    python run_classifier.py --task_name=cola --do_train=true --do_eval=true --do_predict=true --data_dir=./data/ --vocab_file=./cased_L-12_H-768_A-12/vocab.txt --bert_config_file=./cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=./cased_L-12_H-768_A-12/bert_model.ckpt --max_seq_length=280 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=5.0 --output_dir=./bert_output/ --do_lower_case=False\n",
    "    \n",
    "The training process of the model may require some time, depending on our computer's capabilities and the parameters we have chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661c508",
   "metadata": {},
   "source": [
    "The file test_results.tsv is created in the output folder following the prediction process on the test dataset. This file includes the predicted probability values associated with each class label. In case we aim to generate predictions on a new test dataset, test.tsv, we must first complete the model training. Afterward, navigate to the bert_output directory and identify the model.ckpt file with the highest number, as these files hold the weights of the trained model. With the highest checkpoint number in hand, execute the run_classifier.py once more, but this time, set the init_checkpoint to the highest model checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1880e",
   "metadata": {},
   "source": [
    "Then we can type the following command lines.\n",
    "\n",
    "    export TRAINED_MODEL_CKPT=./bert_output/model.ckpt-413\n",
    "\n",
    "    python run_classifier.py --task_name=cola --do_predict=true --data_dir=./data --vocab_file=./cased_L-12_H-768_A-12/vocab.txt --bert_config_file=./cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=$TRAINED_MODEL_CKPT --max_seq_length=128 --output_dir=./bert_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88331f77",
   "metadata": {},
   "source": [
    "## Bert Extractive Summarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b79d5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2f3f9c4a414d0ab775e6705a988bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54054a8c09a465cbc0dfa4f16995638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68241b939e94f44abb2a1348501c132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f422414323146d5aad1ce2267ec6efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\n"
     ]
    }
   ],
   "source": [
    "# Import the Summarizer class from the summarizer module\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# Define a multi-line string variable 'body' containing text\n",
    "body = '''\n",
    "Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can sometimes be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". Other times, they can be more nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".\n",
    "Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step. \n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\n",
    "\n",
    "'''\n",
    "\n",
    "# Instantiate the Summarizer class\n",
    "model = Summarizer()\n",
    "\n",
    "# Call the summarizer model on the 'body' text with a specified minimum summary length (in this case, 60 characters)\n",
    "result = model(body, min_length=60)\n",
    "\n",
    "# Join the resulting summarized text and store it in the 'full' variable\n",
    "full = ''.join(result)\n",
    "\n",
    "# Print the final summarized text\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc974c",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669ad30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " computers learning from data provided so that they carry out certain tasks . for simple tasks assigned to computers , it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand ; on the computer ' s part , no learning is needed . for more advanced tasks , it can be challenging for a human to manually create the needed algorithms . in practice , it can turn out to be more effective to help the machine develop its own algorithm , rather than having human programmers specify every needed step . the discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT model for Question Answering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Define the question and context (body) as strings\n",
    "question = '''What is Machine Learning?'''\n",
    "body = ''' Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can sometimes be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". Other times, they can be more nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".\n",
    "Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step. \n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used. '''\n",
    "\n",
    "# Encode the question and context using the tokenizer\n",
    "encoding = tokenizer.encode_plus(text=question, text_pair=body)\n",
    "\n",
    "# Extract the input IDs (token embeddings) and token type IDs (segment embeddings)\n",
    "inputs = encoding['input_ids']\n",
    "sentence_embedding = encoding['token_type_ids']\n",
    "\n",
    "# Convert input IDs to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs)\n",
    "\n",
    "# Obtain the start and end scores for the answer span from the BERT model\n",
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]), return_dict=False)\n",
    "\n",
    "# Find the indices of the highest start and end scores\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores)\n",
    "\n",
    "# Extract the answer tokens from the token list\n",
    "answer = ' '.join(tokens[start_index:end_index+1])\n",
    "\n",
    "# Initialize an empty string for the corrected answer\n",
    "corrected_answer = ''\n",
    "\n",
    "# Iterate through each word in the answer and correct subword tokens\n",
    "for word in answer.split():\n",
    "    # If it's a subword token, remove the '##' prefix and append to the corrected answer\n",
    "    if word[0:2] == '##':\n",
    "        corrected_answer += word[2:]\n",
    "    # Otherwise, add a space before the word and append to the corrected answer\n",
    "    else:\n",
    "        corrected_answer += ' ' + word\n",
    "\n",
    "# Print the final corrected answer\n",
    "print(corrected_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
