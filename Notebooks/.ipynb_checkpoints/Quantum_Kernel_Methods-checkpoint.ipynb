{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4897c339",
   "metadata": {},
   "source": [
    "# Quantum Kernel Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8108004",
   "metadata": {},
   "source": [
    "To implement our use case, few things need to be considered. As usual, we will import our data, make some preprocessing such as check for missing values, split the data into training and testing datasets, rescale the data using normalization. An important step is feature extraction such as principal component analysis. We need to reduce our data from 43 features to 2 features. We could also use feature selection for dimension reduction. The encoding function is specified in a data_map function (for example data_map_12). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff8f57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading your IBM Quantum account: https://quantum-computing.ibm.com\n",
    "# It requires us to sign with an IBMQ account.\n",
    "#from qiskit import IBMQ\n",
    "#IBMQ.save_account('Y O U R   A P I')\n",
    "\n",
    "# Import utilities\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from functools import reduce\n",
    "\n",
    "# sklearn imports\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "# Importing standard Qiskit libraries and Qiskit Machine Learning imports\n",
    "#from qiskit import BasicAer\n",
    "#from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "#from qiskit_machine_learning.algorithms import QSVC\n",
    "#from qiskit_machine_learning.kernels import QuantumKernel\n",
    "#from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "#from qiskit.algorithms.optimizers import COBYLA\n",
    "#from qiskit.circuit.library import ZZFeatureMap, PauliFeatureMap\n",
    "\n",
    "# Importing utilities for data processing and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Importing scikit-learn utilities for classification and evaluation\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn import metrics\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "# Importing qiskit_machine_learning utilities for classification and evaluation\n",
    "#from qiskit_machine_learning.algorithms import QSVC, PegasosQSVC\n",
    "#from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dc03b",
   "metadata": {},
   "source": [
    "We can choose to compute code with local simulator (Aer simulators) or with online quantum simulators and of course real quantum hardware (Comment / Uncomment the lines below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2500a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute code with local simulator (Aer simulators)\n",
    "from qiskit.primitives import Sampler\n",
    "sampler = Sampler()\n",
    "\n",
    "\n",
    "## Compute code with online quantum simulators or quantum hardware from the cloud\n",
    "## Import QiskitRuntimeService and Sampler\n",
    "#from qiskit_ibm_runtime import QiskitRuntimeService, Sampler\n",
    "## Define service\n",
    "#service = QiskitRuntimeService(channel = 'ibm_quantum', token = ibm_account, instance = 'ibm-q/open/main')\n",
    "## Get backend\n",
    "#quantum_backend = \"ibmq_bogota\"\n",
    "#backend = service.backend(quantum_backend) # Use a simulator or hardware from the cloud\n",
    "## Define Sampler with diferent options\n",
    "## resilience_level=1 adds readout error mitigation\n",
    "## execution.shots is the number of shots\n",
    "## optimization_level=3 adds dynamical decoupling\n",
    "#from qiskit_ibm_runtime import Options\n",
    "#options = Options()\n",
    "#options.resilience_level = 1\n",
    "#options.execution.shots = 1024\n",
    "#options.optimization_level = 3\n",
    "#sampler = Sampler(session=backend, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04bc53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for randomization, to keep outputs consistent\n",
    "from qiskit.utils import algorithm_globals\n",
    "seed = 123456\n",
    "algorithm_globals.random_seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdac5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "from functools import reduce\n",
    "\n",
    "def data_map_8(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*(m * n), x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_9(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: (np.pi/2)*(m * n), 1 - x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_10(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*np.exp(((n - m)*(n - m))/8), x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_11(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: (np.pi/3)*(m * n), 1/(np.cos(x)))\n",
    "    return coeff\n",
    "\n",
    "def data_map_12(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*(m * n), np.cos(x))\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d354c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n",
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n",
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n",
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n",
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n",
      "     ┌─────────────────────────────┐\n",
      "q_0: ┤0                            ├\n",
      "     │  PauliFeatureMap(x[0],x[1]) │\n",
      "q_1: ┤1                            ├\n",
      "     └─────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Quantum Feature Mapping with feature_dimension = 2 and reps = 2\n",
    "from qiskit.circuit.library import PauliFeatureMap\n",
    "\n",
    "qfm_default = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full')\n",
    "print(qfm_default)\n",
    "qfm_8 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_8)\n",
    "print(qfm_8)\n",
    "qfm_9 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_9)\n",
    "print(qfm_9)\n",
    "qfm_10 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_10)\n",
    "print(qfm_10)\n",
    "qfm_11 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_11)\n",
    "print(qfm_11)\n",
    "qfm_12 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_12)\n",
    "print(qfm_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcf519",
   "metadata": {},
   "source": [
    "We utilize the default implementation of the Sampler primitive and the ComputeUncompute fidelity, which calculates the overlaps between states.If you do not provide specific instances of Sampler or Fidelity, the code will automatically create these objects with the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ff463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "Q_Kernel_8 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_8)\n",
    "Q_Kernel_9 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_9)\n",
    "Q_Kernel_10 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_10)\n",
    "Q_Kernel_11 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_11)\n",
    "Q_Kernel_12 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_12)\n",
    "Q_Kernel_default = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5186048",
   "metadata": {},
   "source": [
    "We process our data as we did for classical computing (load data, missing data, split of the data, normalization, PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07693847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PCA_1     PCA_2\n",
      "0   -2.246856  1.001576\n",
      "1   10.456852 -0.864867\n",
      "2   -6.478235 -0.469133\n",
      "3   -1.196812 -1.323666\n",
      "4   -6.189865 -1.503502\n",
      "5   -6.064777 -1.038960\n",
      "6    2.588510 -2.702076\n",
      "7   -3.618969  2.981614\n",
      "8    0.310268 -2.836980\n",
      "9   -3.018028  0.251393\n",
      "10  -1.423122  1.351361\n",
      "11   3.969591  0.555156\n",
      "12  -0.698979 -0.667258\n",
      "13   5.008537  2.118542\n",
      "14  -0.547348  1.042548\n",
      "15  -6.022106 -1.572483\n",
      "16   2.714648  8.179320\n",
      "17   1.170691 -2.017913\n",
      "18  -5.431129 -0.880815\n",
      "19   1.283781 -1.080195\n",
      "20  -4.300506 -0.645016\n",
      "21  -2.486374 -0.284906\n",
      "22   5.892424  0.779772\n",
      "23  -4.295756  0.346126\n",
      "24  -2.788814  3.233115\n",
      "25  -6.570051 -1.796226\n",
      "26   7.748237 -1.149488\n",
      "27  -5.325899 -0.909251\n",
      "28   2.388197  5.516065\n",
      "29   6.763665 -2.122499\n",
      "30  -1.684444  1.497672\n",
      "31  11.773240  0.039079\n",
      "32   2.614585 -3.518442\n",
      "33   9.197619 -2.454461\n",
      "34  -3.492777  0.944796\n",
      "       PCA_1     PCA_2\n",
      "0  -2.828170 -2.752159\n",
      "1   1.331436 -1.650711\n",
      "2  11.925533 -1.371046\n",
      "3   2.394277 -0.203664\n",
      "4  -6.883203  0.030448\n",
      "5   4.064617  1.760038\n",
      "6  -6.971722  1.216491\n",
      "7   2.413532  3.685556\n",
      "8  -5.446301 -0.714953\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import dataset\n",
    "data = '../data/datasets/neurons_binary.csv'\n",
    "neuron = pd.read_csv(data, delimiter=';')\n",
    "\n",
    "df = neuron.head(22).copy()                        # Principal\n",
    "df = pd.concat([df, neuron.iloc[17034:17056]])     # Interneuron\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating an instance of Labelencoder\n",
    "enc = LabelEncoder()\n",
    "# Assigning numerical value and storing it\n",
    "df[[\"Target\"]] = df[[\"Target\"]].apply(enc.fit_transform)\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = X.columns)\n",
    "\n",
    "# Dimension Reduction with PCA (with two principal compoenents)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "# transform data\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "# Define a new DataFrame with two column (the principal components)\n",
    "component_columns = []\n",
    "for x in (n+1 for n in range(2)):\n",
    "    component_columns = component_columns + ['PCA_%i'%x]\n",
    "X_train = pd.DataFrame(data = X_train, columns = component_columns)\n",
    "X_test = pd.DataFrame(data = X_test, columns = component_columns)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee579d",
   "metadata": {},
   "source": [
    "Then, we iterate the different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b6565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "names = [\"Q_Kernel_default\", \"Q_Kernel_8\", \"Q_Kernel_9\",\n",
    "         \"Q_Kernel_10\", \"Q_Kernel_11\", \"Q_Kernel_12\"]\n",
    "\n",
    "classifiers = [\n",
    "    QSVC(quantum_kernel=Q_Kernel_default),\n",
    "    QSVC(quantum_kernel=Q_Kernel_8),\n",
    "    QSVC(quantum_kernel=Q_Kernel_9),\n",
    "    QSVC(quantum_kernel=Q_Kernel_10),\n",
    "    QSVC(quantum_kernel=Q_Kernel_11),\n",
    "    QSVC(quantum_kernel=Q_Kernel_12),\n",
    "              ]\n",
    "\n",
    "#model = QSVC(quantum_kernel=Q_Kernel_8)\n",
    "\n",
    "#classifiers = [\n",
    "#    SVC(kernel=Q_Kernel_default.evaluate),\n",
    "#    SVC(kernel=Q_Kernel_8.evaluate),\n",
    "#    SVC(kernel=Q_Kernel_9.evaluate),\n",
    "#    SVC(kernel=Q_Kernel_10.evaluate),\n",
    "#    SVC(kernel=Q_Kernel_11.evaluate),\n",
    "#    SVC(kernel=Q_Kernel_12.evaluate),\n",
    "#              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7121ee94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callable kernel classification test score for Q_Kernel_default: 0.3333333333333333\n",
      "Callable kernel classification test score for Q_Kernel_8: 0.4444444444444444\n",
      "Callable kernel classification test score for Q_Kernel_9: 0.4444444444444444\n",
      "Callable kernel classification test score for Q_Kernel_10: 0.6666666666666666\n",
      "Callable kernel classification test score for Q_Kernel_11: 0.6666666666666666\n",
      "Callable kernel classification test score for Q_Kernel_12: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Iterate over classifiers and give classification test score\n",
    "for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(f'Callable kernel classification test score for {name}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61486b02",
   "metadata": {},
   "source": [
    "As we also did previously in classical computing, we can provide metrics about our model (Accuracy, Precision, Recall, F1 score, cross validation or use the classification report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e26c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q_Kernel_default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[1 1 0 1 0 1 1 1 0]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.3333333333333333\n",
      "f1 Score: 0.3333333333333333\n",
      "Cross Validation Mean: 0.5142857142857142\n",
      "Cross Validation Std: 0.06998542122237651\n",
      "\n",
      "\n",
      "Q_Kernel_8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[1 1 1 0 1 1 0 0 0]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.4444444444444444\n",
      "Precision: 0.4444444444444444\n",
      "Recall: 0.4444444444444444\n",
      "f1 Score: 0.4444444444444444\n",
      "Cross Validation Mean: 0.7142857142857142\n",
      "Cross Validation Std: 0.18070158058105024\n",
      "\n",
      "\n",
      "Q_Kernel_9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[0 1 1 0 0 0 0 0 1]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.4444444444444444\n",
      "Precision: 0.4444444444444444\n",
      "Recall: 0.4444444444444444\n",
      "f1 Score: 0.4444444444444444\n",
      "Cross Validation Mean: 0.6571428571428571\n",
      "Cross Validation Std: 0.23211538298959886\n",
      "\n",
      "\n",
      "Q_Kernel_10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[0 0 0 1 1 0 1 1 1]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6666666666666666\n",
      "f1 Score: 0.6666666666666666\n",
      "Cross Validation Mean: 0.6571428571428571\n",
      "Cross Validation Std: 0.06998542122237654\n",
      "\n",
      "\n",
      "Q_Kernel_11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[1 1 1 0 1 1 1 0 1]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6666666666666666\n",
      "f1 Score: 0.6666666666666666\n",
      "Cross Validation Mean: 0.45714285714285713\n",
      "Cross Validation Std: 0.05714285714285714\n",
      "\n",
      "\n",
      "Q_Kernel_12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print predicted data coming from X_test as new input data\n",
      "[1 1 0 0 0 0 0 0 1]\n",
      "\n",
      "\n",
      "Print real values\n",
      "\n",
      "16       1\n",
      "17038    0\n",
      "17052    0\n",
      "17040    0\n",
      "17       1\n",
      "17046    0\n",
      "9        1\n",
      "17049    0\n",
      "21       1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6666666666666666\n",
      "f1 Score: 0.6666666666666666\n",
      "Cross Validation Mean: 0.5714285714285714\n",
      "Cross Validation Std: 0.18070158058105024\n",
      "Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.67      0.65      0.65         9\n",
      "weighted avg       0.67      0.67      0.66         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Provide metrics over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "        print(\"\\n\")\n",
    "        print(name)\n",
    "        print(\"\\n\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "    \n",
    "        print(\"\\n\")\n",
    "        print(\"Print predicted data coming from X_test as new input data\")\n",
    "        print(y_pred)\n",
    "        print(\"\\n\")\n",
    "        print(\"Print real values\\n\")\n",
    "        print(y_test)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "        print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='micro'))\n",
    "        print(\"f1 Score:\", metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "        print(\"Cross Validation Mean:\", cross_val_score(clf, X_train, y_train, cv=5).mean())\n",
    "        print(\"Cross Validation Std:\", cross_val_score(clf, X_train, y_train, cv=5).std())\n",
    "\n",
    "print('Classification Report: \\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59e7a1",
   "metadata": {},
   "source": [
    "## q_kernel_zz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7f542",
   "metadata": {},
   "source": [
    "Another coding example with q_kernel_zz is described below. We will use the ZZFeatureMap with linear entanglement, we will repeat the data encoding step two times, and we will use feature selection (embedded decision tree) to select five features (on a very small dataset of 260 neurons). We will use five qubits on the StatevectorSimulator from the IBM Quantum framework (https://qiskit.org/). The simulator models the noiseless execution of quantum computer hardware, representing the ideal. It evaluates the resulting quantum state vector. For each experiment, to assess whether patterns are identifiable, we will train the supervised classification algorithms using 80% of each sample, randomly chosen, and we will assess the accuracy in predicting the remaining 20%. The accuracy of the classification will be assessed by performing cross-validation on the training dataset. We will average the 5-fold cross-validation scores resulting in a mean +/- standard deviation score. For data rescaling, we will use QuantileTransformer. We also will transform the dataset using a Mahalanobis transformation with suppression of a neuron when the surface of the soma equals 0. The Mahalanobis distance is a multivariate metric measuring the distance between a point and a distribution. Applying the Mahalonobis distance allows reduction of the standard deviation for each feature by deleting neurons from the dataset. The datasets will be preprocessed to address missing values. If a value within the features is missing, the neuron will be deleted from the dataset. Categorical features such as morphology types will be encoded, transforming each categorical feature with m possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55b534f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Target  Soma_Surface  N_stems  N_bifs  N_branch  N_tips  \\\n",
      "0            ganglion      1149.320      4.0   101.0     206.0   106.0   \n",
      "1            ganglion      1511.830      3.0    70.0     143.0    74.0   \n",
      "2            ganglion      1831.530      3.0    13.0      29.0    17.0   \n",
      "3            ganglion      1291.270      6.0   109.0     224.0   116.0   \n",
      "4            ganglion      3064.340      4.0    60.0     124.0    65.0   \n",
      "...               ...           ...      ...     ...       ...     ...   \n",
      "22686  double_bouquet       605.067      5.0   132.0     269.0   138.0   \n",
      "22687  double_bouquet       920.949      6.0   121.0     248.0   128.0   \n",
      "22688  double_bouquet       770.529      3.0   104.0     211.0   108.0   \n",
      "22689  double_bouquet       478.078      4.0   158.0     320.0   163.0   \n",
      "22690  double_bouquet       629.470      4.0    65.0     134.0    70.0   \n",
      "\n",
      "        Width   Height   Depth     Type  ...  Bif_ampl_remote  Bif_tilt_local  \\\n",
      "0      249.09   493.80   33.63   1806.0  ...         9132.460         9543.61   \n",
      "1      453.80   390.94   35.00   1504.0  ...         6034.870         6750.98   \n",
      "2      282.23   324.06   29.24    699.0  ...          835.754         1215.33   \n",
      "3      228.86   616.17   42.19   2131.0  ...         9696.640        10160.10   \n",
      "4      264.09   364.24   44.37   1568.0  ...         5084.620         5927.53   \n",
      "...       ...      ...     ...      ...  ...              ...             ...   \n",
      "22686  222.40  1212.65  120.80   6247.0  ...        11348.400        13220.00   \n",
      "22687  328.02   980.65  227.04  32561.0  ...         9996.370        12886.10   \n",
      "22688  247.39  1322.26   96.81   5746.0  ...         7972.950        10343.50   \n",
      "22689  343.21   813.14  115.01   9878.0  ...        11555.700        15740.30   \n",
      "22690  305.82  1164.29  183.69  15343.0  ...         5267.240         6223.78   \n",
      "\n",
      "       Bif_tilt_remote  Bif_torque_local  Bif_torque_remote  Last_parent_diam  \\\n",
      "0              9714.80           7413.60            7348.63             11.24   \n",
      "1              6781.43           6896.19            7620.16              7.75   \n",
      "2              1478.25           1192.02            1001.38              4.06   \n",
      "3             10281.70           9992.69           10400.00              9.38   \n",
      "4              5852.84           5128.55            5371.86              7.73   \n",
      "...                ...               ...                ...               ...   \n",
      "22686         12887.70          12790.80           12031.90             11.41   \n",
      "22687         12170.90          10669.40           10279.20             28.52   \n",
      "22688         10984.10           9389.51           10205.70             10.84   \n",
      "22689         17768.70          14536.90           13063.80             17.55   \n",
      "22690          6298.83           5738.15            6259.83              6.12   \n",
      "\n",
      "       Diam_threshold  HillmanThreshold  Helix  Fractal_Dim  \n",
      "0            114.0480          123.3370  -0.33      56.4171  \n",
      "1            101.5980          103.2830  -0.01      46.9175  \n",
      "2             59.7340           72.2750  -0.64      20.7496  \n",
      "3            125.3340          145.4360  -5.34      49.3177  \n",
      "4            123.6620          140.4000  -1.27      51.0624  \n",
      "...               ...               ...    ...          ...  \n",
      "22686         73.7220           78.2270  -1.86     205.7210  \n",
      "22687         63.8622           95.7028   1.93     239.6990  \n",
      "22688         71.3840           79.0890  -2.71     142.2360  \n",
      "22689         73.8240           81.1340  -3.20     271.7430  \n",
      "22690         44.6384           53.5180 -11.67     124.9970  \n",
      "\n",
      "[22691 rows x 44 columns]\n",
      "\n",
      "\n",
      "Decision Tree Regressor Features Importance: started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Features Importances:\n",
      "\n",
      "\n",
      "                 Features  Importances\n",
      "0            Soma_Surface     0.045248\n",
      "1                 N_stems     0.019728\n",
      "2                  N_bifs     0.000000\n",
      "3                N_branch     0.000000\n",
      "4                  N_tips     0.000000\n",
      "5                   Width     0.000000\n",
      "6                  Height     0.169403\n",
      "7                   Depth     0.152039\n",
      "8                    Type     0.009724\n",
      "9                Diameter     0.000000\n",
      "10           Diameter_pow     0.000000\n",
      "11                 Length     0.055116\n",
      "12                Surface     0.000000\n",
      "13            SectionArea     0.013165\n",
      "14                 Volume     0.000000\n",
      "15            EucDistance     0.000000\n",
      "16           PathDistance     0.069828\n",
      "17           Branch_Order     0.084260\n",
      "18        Terminal_degree     0.006915\n",
      "19        TerminalSegment     0.048528\n",
      "20                Taper_1     0.076445\n",
      "21                Taper_2     0.008298\n",
      "22      Branch_pathlength     0.007780\n",
      "23            Contraction     0.000000\n",
      "24          Fragmentation     0.009681\n",
      "25         Daughter_Ratio     0.024061\n",
      "26  Parent_Daughter_Ratio     0.000000\n",
      "27    Partition_asymmetry     0.000000\n",
      "28             Rall_Power     0.000000\n",
      "29                     Pk     0.000000\n",
      "30             Pk_classic     0.006915\n",
      "31                   Pk_2     0.000000\n",
      "32         Bif_ampl_local     0.061259\n",
      "33        Bif_ampl_remote     0.032953\n",
      "34         Bif_tilt_local     0.000000\n",
      "35        Bif_tilt_remote     0.000000\n",
      "36       Bif_torque_local     0.005186\n",
      "37      Bif_torque_remote     0.000000\n",
      "38       Last_parent_diam     0.026084\n",
      "39         Diam_threshold     0.052630\n",
      "40       HillmanThreshold     0.000000\n",
      "41                  Helix     0.014752\n",
      "42            Fractal_Dim     0.000000\n",
      "\n",
      "\n",
      "Decision Tree Classifier Features Importance: DataFrame\n",
      "\n",
      "\n",
      "       Height     Depth  Branch_Order   Taper_1  PathDistance\n",
      "0    0.980769  0.586538      0.725962  0.725962      0.774038\n",
      "1    0.697115  0.725962      0.769231  0.850962      0.730769\n",
      "2    0.533654  0.206731      0.437500  0.423077      0.461538\n",
      "3    0.105769  0.076923      0.324519  0.932692      0.187500\n",
      "4    0.235577  0.538462      0.485577  0.375000      0.495192\n",
      "..        ...       ...           ...       ...           ...\n",
      "204  0.658654  0.437500      0.480769  0.673077      0.504808\n",
      "205  0.947115  0.822115      0.879808  0.923077      0.889423\n",
      "206  0.278846  0.000000      0.033654  0.014423      0.062500\n",
      "207  0.480769  0.841346      0.951923  0.894231      0.918269\n",
      "208  0.817308  0.639423      0.932692  0.870192      0.879808\n",
      "\n",
      "[209 rows x 5 columns]\n",
      "     ┌─────────────────────────────────────────┐\n",
      "q_0: ┤0                                        ├\n",
      "     │                                         │\n",
      "q_1: ┤1                                        ├\n",
      "     │                                         │\n",
      "q_2: ┤2 ZZFeatureMap(x[0],x[1],x[2],x[3],x[4]) ├\n",
      "     │                                         │\n",
      "q_3: ┤3                                        ├\n",
      "     │                                         │\n",
      "q_4: ┤4                                        ├\n",
      "     └─────────────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:2590: UserWarning: n_quantiles (1000) is greater than the total number of samples (209). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:2590: UserWarning: n_quantiles (1000) is greater than the total number of samples (53). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callable kernel classification test score for q_kernel_zz: 0.5471698113207547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/4vnghq3579l_6xbflfl1hk180000gn/T/ipykernel_215/267321166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Callable kernel classification test score for q_kernel_zz: {score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_compute_kernel\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;31m# in the case of precomputed kernel given as a function, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# have to compute explicitly the kernel matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__Xfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x_vec, y_vec)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mleft_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_parameterization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             kernel_matrix = self._get_kernel_matrix(\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0mkernel_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py\u001b[0m in \u001b[0;36m_get_kernel_matrix\u001b[0;34m(self, kernel_shape, left_parameters, right_parameters, indices)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparameterization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mcomputes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msymmetric\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mkernel_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kernel_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# fill in trivial entries and then update with fidelity values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py\u001b[0m in \u001b[0;36m_get_kernel_entries\u001b[0;34m(self, left_parameters, right_parameters)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mright_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mkernel_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfidelities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# trivial case, only identical samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/qiskit/primitives/primitive_job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;34m\"\"\"Return the results of the job.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_submitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading your IBM Quantum account(s)\n",
    "#from qiskit import IBMQ\n",
    "#from qiskit.providers.ibmq import least_busy\n",
    "\n",
    "# Import utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn import metrics\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "# Importing standard Qiskit libraries and Qiskit Machine Learning imports\n",
    "#from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "#from qiskit.circuit.library import ZZFeatureMap\n",
    "#from qiskit_machine_learning.algorithms import QSVC\n",
    "#from qiskit import QuantumCircuit\n",
    "#from qiskit.circuit import ParameterVector\n",
    "#from qiskit.visualization import circuit_drawer\n",
    "\n",
    "#from typing import Union\n",
    "#from qiskit_machine_learning.exceptions import QiskitMachineLearningError\n",
    "\n",
    "# seed for randomization, to keep outputs consistent\n",
    "from qiskit.utils import algorithm_globals\n",
    "seed = 123456\n",
    "algorithm_globals.random_seed = seed\n",
    "\n",
    "# Define parameters\n",
    "cv = 5 # 5-fold cross-validation \n",
    "feature_dimension = 5 # Features dimension\n",
    "k_features = 5 # Feature selection\n",
    "reps = 2 # Repetition\n",
    "ibm_account = 'YOUR API'\n",
    "quantum_backend = 'simulator_statevector'\n",
    "\n",
    "# Import dataset\n",
    "data = '../data/datasets/neurons_maha_soma.csv'\n",
    "neuron = pd.read_csv(data, delimiter=',')\n",
    "\n",
    "print(neuron)\n",
    "\n",
    "df = neuron.head(22).copy()                    # Ganglion\n",
    "df = pd.concat([df, neuron.iloc[320:340]])     # Granule\n",
    "df = pd.concat([df, neuron.iloc[1493:1513]])   # Medium Spiny\n",
    "df = pd.concat([df, neuron.iloc[1171:1191]])   # Parachromaffin\n",
    "df = pd.concat([df, neuron.iloc[10031:10051]])   # Pyramidal\n",
    "\n",
    "df = pd.concat([df, neuron.iloc[2705:2725]]) # Basket\n",
    "df = pd.concat([df, neuron.iloc[22589:22609]]) # Bitufted\n",
    "df = pd.concat([df, neuron.iloc[3175:3195]]) # Chandelier\n",
    "df = pd.concat([df, neuron.iloc[22644:22664]]) # Double bouquet\n",
    "df = pd.concat([df, neuron.iloc[3199:3219]]) # Martinotti\n",
    "df = pd.concat([df, neuron.iloc[8260:8280]]) # Nitrergic\n",
    "\n",
    "df = pd.concat([df, neuron.iloc[2255:2275]]) # Astrocytes\n",
    "df = pd.concat([df, neuron.iloc[3306:3326]]) # Microglia\n",
    "\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating an instance of Labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "# Assigning numerical value and storing it\n",
    "df[[\"Target\"]] = df[[\"Target\"]].apply(enc.fit_transform)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We split our data to y (Target) and X (features)\n",
    "y = df.loc[:, df.columns == 'Target']\n",
    "# Features variables\n",
    "X = df.loc[:, df.columns != ('Target')]\n",
    "# Split data into train and test\n",
    "# Option test_size = 0.2 means that we take 20% of the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "Normalize = QuantileTransformer(n_quantiles=1000, output_distribution=\"uniform\")\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = X.columns)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def embedded_decision_tree_classifier(X, y, k_features, output_folder=None):\n",
    "    '''\n",
    "    Here we use decision tree classifier to select features. We select the k best features (k_features)\n",
    "    \n",
    "    Inputs:\n",
    "        - X (features) DataFrame\n",
    "        - y (target) DataFrame\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Decision Tree Regressor Features Importance: started\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # define the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # get importance\n",
    "    importance = model.feature_importances_\n",
    "    # Get features name\n",
    "    feature_names = [f\"{i}\" for i in X.columns]\n",
    "\n",
    "    # create a data frame to visualize features importance\n",
    "    features_importance = pd.DataFrame({\"Features\": feature_names, \"Importances\":importance})\n",
    "    features_importance.set_index('Importances')\n",
    "\n",
    "    # Print features importance\n",
    "    print(\"\\n\")\n",
    "    print(\"Features Importances:\")\n",
    "    print(\"\\n\")\n",
    "    print(features_importance)\n",
    "    if output_folder is not None:\n",
    "        features_importance.to_csv(output_folder+'Decision_Tree_Classifier_Features_Importance.csv', index=False)\n",
    "\n",
    "    if output_folder is not None:\n",
    "        # plot feature importance\n",
    "        features_importance.plot(kind='bar',x='Features',y='Importances')\n",
    "        pyplot.title('Decision Tree Classifier Features Importance')\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.savefig(output_folder+'Decision_Tree_Classfier_Features_Importance.png')\n",
    "    \n",
    "    # Select the k most important features\n",
    "    features_columns = []\n",
    "    # Order the features importance dataframe\n",
    "    df = pd.DataFrame(data = features_importance.sort_values(by='Importances', key=abs,ascending=False))\n",
    "    # Put the k most important features in features_columns\n",
    "    for x in range(k_features):\n",
    "        features_columns = features_columns + [df.iloc[x][0]]\n",
    "\n",
    "    # Create a new DataFrame with selected features\n",
    "    df_data = pd.DataFrame(data = X, columns = features_columns)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Decision Tree Classifier Features Importance: DataFrame\")\n",
    "    print(\"\\n\")\n",
    "    print(df_data)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "X_train = embedded_decision_tree_classifier(X_train, y_train, k_features)\n",
    "X_test = pd.DataFrame(data = X_test, columns = X_train.columns)\n",
    "\n",
    "feature_dimension = X_train.shape[1] # Number of features\n",
    "multiclass = None\n",
    "output_folder = None\n",
    "   \n",
    "# We convert pandas DataFrame into numpy array\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# seed for randomization, to keep outputs consistent\n",
    "seed = 123456\n",
    "algorithm_globals.random_seed = seed\n",
    "\n",
    "# Quantum Feature Mapping with feature_dimension = 5 and reps = 2\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "qfm_zz = ZZFeatureMap(feature_dimension=feature_dimension, reps=reps, entanglement=\"linear\")\n",
    "            \n",
    "print(qfm_zz)\n",
    "\n",
    "if quantum_backend is not None:\n",
    "    # Compute code with online quantum simulators or quantum hardware from the cloud\n",
    "    # Import QiskitRuntimeService and Sampler\n",
    "    from qiskit_ibm_runtime import QiskitRuntimeService, Sampler\n",
    "    # Define service\n",
    "    service = QiskitRuntimeService(channel = 'ibm_quantum', token = ibm_account, instance = 'ibm-q/open/main')\n",
    "    # Get backend\n",
    "    backend = service.backend(quantum_backend) # Use a simulator or hardware from the cloud\n",
    "    # Define Sampler with diferent options\n",
    "    # resilience_level=1 adds readout error mitigation\n",
    "    # execution.shots is the number of shots\n",
    "    # optimization_level=3 adds dynamical decoupling\n",
    "    from qiskit_ibm_runtime import Options\n",
    "    options = Options()\n",
    "    options.resilience_level = 1\n",
    "    options.execution.shots = 1024\n",
    "    options.optimization_level = 3\n",
    "    sampler = Sampler(session=backend, options = options)\n",
    "else:\n",
    "    # Compute code with local simulator (Aer simulators)\n",
    "    from qiskit.primitives import Sampler\n",
    "    sampler = Sampler()\n",
    "    \n",
    "# After preparing our training and testing datasets, we configure the FidelityQuantumKernel class to compute a kernel matrix using the ZZFeatureMap.\n",
    "# We utilize the default implementation of the Sampler primitive and the ComputeUncompute fidelity, which calculates the overlaps between states.\n",
    "# If you do not provide specific instances of Sampler or Fidelity, the code will automatically create these objects with the default values.\n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "Q_Kernel_zz = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_zz)\n",
    "\n",
    "\n",
    "# QSVC model\n",
    "model = QSVC(quantum_kernel=Q_Kernel_zz)\n",
    "model.fit(X_train,y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Callable kernel classification test score for q_kernel_zz: {score}')\n",
    "          \n",
    "y_pred = model.predict(X_test)\n",
    "        \n",
    "print(\"\\n\")\n",
    "print(\"Print predicted data coming from X_test as new input data\")\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Print real values\\n\")\n",
    "print(y_test)\n",
    "print(\"\\n\")\n",
    "    \n",
    "# K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=cv)\n",
    "score = np.zeros(cv)\n",
    "i = 0\n",
    "print(score)\n",
    "for indices_train, indices_test in k_fold.split(X_train):\n",
    "    #print(indices_train, indices_test)\n",
    "    X_train_ = X_train[indices_train]\n",
    "    X_test_ = X_train[indices_test]\n",
    "    y_train_ = y_train[indices_train]\n",
    "    y_test_ = y_train[indices_test]\n",
    " \n",
    "    # fit classifier to data\n",
    "    model.fit(X_train_, y_train_)\n",
    "\n",
    "    # score classifier\n",
    "    score[i] = model.score(X_test_, y_test_)\n",
    "    i = i + 1\n",
    "\n",
    "import math\n",
    "print(\"cross validation scores: \", score)\n",
    "cross_mean = sum(score) / len(score)\n",
    "cross_var = sum(pow(x - cross_mean,2) for x in score) / len(score)  # variance\n",
    "cross_std  = math.sqrt(cross_var)  # standard deviation\n",
    "print(\"cross validation mean: \", cross_mean)\n",
    "    \n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_mean, cross_std]\n",
    "    \n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns=['q_kernel_zz'])\n",
    "\n",
    "print('Classification Report: \\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "        \n",
    "print(metrics_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42a739",
   "metadata": {},
   "source": [
    "# Pegasos Quantum Support Vector Classifier: Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358873e",
   "metadata": {},
   "source": [
    "There is also an alternative method to QSVC (which use the dual optimization from scikit-learn) using the Pegasos algorithm from Shalev-Shwartz where another SVM based algorithm benefits from the quantum kernel method. PegasosQSVC yields a training complexity that is independent of the size of the training set. This means that it could train faster than QSVC with large training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading your IBM Quantum account(s)\n",
    "#from qiskit import IBMQ\n",
    "#IBMQ.save_account('YOUR API')\n",
    "\n",
    "# Import utilities\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from functools import reduce\n",
    "\n",
    "# sklearn imports\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "# Importing standard Qiskit libraries and Qiskit Machine Learning imports\n",
    "#from qiskit import BasicAer\n",
    "#from qiskit.circuit.library import ZZFeatureMap, PauliFeatureMap\n",
    "#from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "#from qiskit_machine_learning.algorithms import QSVC\n",
    "#from qiskit_machine_learning.algorithms import PegasosQSVC\n",
    "#from qiskit_machine_learning.kernels import QuantumKernel\n",
    "#from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "#from qiskit.algorithms.optimizers import COBYLA\n",
    "\n",
    "# Backend objects can also be set up using the IBMQ package.\n",
    "# The use of these requires us to sign with an IBMQ account.\n",
    "# Assuming the credentials are already loaded onto your computer, you sign in with\n",
    "#IBMQ.load_account()\n",
    "#provider = IBMQ.get_provider(hub='ibm-q')\n",
    "#backend = provider.get_backend('ibmq_lima')\n",
    "\n",
    "# What additional backends we have available.\n",
    "#for backend in provider.backends():\n",
    "#    print(backend)\n",
    "    \n",
    "# seed for randomization, to keep outputs consistent\n",
    "from qiskit.utils import algorithm_globals\n",
    "seed = 123456\n",
    "algorithm_globals.random_seed = seed\n",
    "\n",
    "# number of qubits is equal to the number of features\n",
    "#num_qubits = 2\n",
    "\n",
    "# number of steps performed during the training procedure\n",
    "tau = 100\n",
    "\n",
    "# regularization parameter\n",
    "C = 1000\n",
    "\n",
    "# Encoding Functions\n",
    "from functools import reduce\n",
    "\n",
    "def data_map_8(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*(m * n), x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_9(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: (np.pi/2)*(m * n), 1 - x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_10(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*np.exp(((n - m)*(n - m))/8), x)\n",
    "    return coeff\n",
    "\n",
    "def data_map_11(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: (np.pi/3)*(m * n), 1/(np.cos(x)))\n",
    "    return coeff\n",
    "\n",
    "def data_map_12(x: np.ndarray) -> float:\n",
    "    coeff = x[0] if len(x) == 1 else reduce(lambda m, n: np.pi*(m * n), np.cos(x))\n",
    "    return coeff\n",
    "    \n",
    "# Quantum Feature Mapping with feature_dimension = 2 and reps = 2\n",
    "from qiskit.circuit.library import PauliFeatureMap\n",
    "\n",
    "qfm_default = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full')\n",
    "print(qfm_default)\n",
    "qfm_8 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_8)\n",
    "print(qfm_8)\n",
    "qfm_9 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_9)\n",
    "print(qfm_9)\n",
    "qfm_10 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_10)\n",
    "print(qfm_10)\n",
    "qfm_11 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_11)\n",
    "print(qfm_11)\n",
    "qfm_12 = PauliFeatureMap(feature_dimension=2,\n",
    "                                    paulis = ['ZI','IZ','ZZ'],\n",
    "                                 reps=2, entanglement='full', data_map_func=data_map_12)\n",
    "print(qfm_12)\n",
    "                                 \n",
    "print(qfm_8.draw())\n",
    "\n",
    "# For quantum access, the following lines must be adapted\n",
    "if quantum_backend is not None:\n",
    "    # Compute code with online quantum simulators or quantum hardware from the cloud\n",
    "    # Import QiskitRuntimeService and Sampler\n",
    "    from qiskit_ibm_runtime import QiskitRuntimeService, Sampler\n",
    "    # Define service\n",
    "    service = QiskitRuntimeService(channel = 'ibm_quantum', token = ibm_account, instance = 'ibm-q/open/main')\n",
    "    # Get backend\n",
    "    backend = service.backend(quantum_backend) # Use a simulator or hardware from the cloud\n",
    "    # Define Sampler: We use the reference implementation of the Sampler primitive and the ComputeUncompute fidelity that computes overlaps between states. These are the default values and if you don't pass a Sampler or Fidelity instance, the same objects will be created automatically for you.\n",
    "    # Run Quasi-Probability calculation\n",
    "    # optimization_level=3 adds dynamical decoupling\n",
    "    # resilience_level=1 adds readout error mitigation\n",
    "    from qiskit_ibm_runtime import Options\n",
    "    options = Options()\n",
    "    options.resilience_level = 1\n",
    "    options.execution.shots = 1024\n",
    "    options.optimization_level = 3\n",
    "    sampler = Sampler(session=backend, options = options)\n",
    "else:\n",
    "    # Compute code with local simulator (Aer simulators)\n",
    "    from qiskit.primitives import Sampler\n",
    "    sampler = Sampler()\n",
    "    \n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "Q_Kernel_default = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_default)\n",
    "Q_Kernel_8 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_8)\n",
    "Q_Kernel_9 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_9)\n",
    "Q_Kernel_10 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_10)\n",
    "Q_Kernel_11 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_11)\n",
    "Q_Kernel_12 = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm_12)\n",
    "\n",
    "names = [\"Q_Kernel_default\", \"Q_Kernel_8\", \"Q_Kernel_9\",\n",
    "         \"Q_Kernel_10\", \"Q_Kernel_11\", \"Q_Kernel_12\"]\n",
    "\n",
    "classifiers = [\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_default, C=C, num_steps=tau),\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_8, C=C, num_steps=tau),\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_9, C=C, num_steps=tau),\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_10, C=C, num_steps=tau),\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_11, C=C, num_steps=tau),\n",
    "    PegasosQSVC(quantum_kernel=Q_Kernel_12, C=C, num_steps=tau),\n",
    "              ]\n",
    "\n",
    "# Import dataset\n",
    "data = '../data/datasets/neurons_binary.csv'\n",
    "neuron = pd.read_csv(data, delimiter=';')\n",
    "\n",
    "df = neuron.head(22).copy()                        # Principal\n",
    "df = pd.concat([df, neuron.iloc[17034:17056]])     # Interneuron\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating an instance of Labelencoder\n",
    "enc = LabelEncoder()\n",
    "# Assigning numerical value and storing it\n",
    "df[[\"Target\"]] = df[[\"Target\"]].apply(enc.fit_transform)\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Scaling the data\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = X.columns)\n",
    "\n",
    "# Dimension Reduction with PCA (with two principal compoenents)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "# transform data\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "# Define a new DataFrame with two column (the principal components)\n",
    "component_columns = []\n",
    "for x in (n+1 for n in range(2)):\n",
    "    component_columns = component_columns + ['PCA_%i'%x]\n",
    "X_train = pd.DataFrame(data = X_train, columns = component_columns)\n",
    "X_test = pd.DataFrame(data = X_test, columns = component_columns)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(f'Callable kernel classification test score for {name}: {score}')\n",
    "        \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Provide metrics over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "        print(\"\\n\")\n",
    "        print(name)\n",
    "        print(\"\\n\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "    \n",
    "        print(\"\\n\")\n",
    "        print(\"Print predicted data coming from X_test as new input data\")\n",
    "        print(y_pred)\n",
    "        print(\"\\n\")\n",
    "        print(\"Print real values\\n\")\n",
    "        print(y_test)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "        print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='micro'))\n",
    "        print(\"f1 Score:\", metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "        print(\"Cross Validation Mean:\", cross_val_score(clf, X_train, y_train, cv=5).mean())\n",
    "        print(\"Cross Validation Std:\", cross_val_score(clf, X_train, y_train, cv=5).std())\n",
    "        \n",
    "        \n",
    "        results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(clf, X_train, y_train, cv=5).mean(), cross_val_score(clf, X_train, y_train, cv=5).std()]\n",
    "        metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns=[name])\n",
    "\n",
    "        print('Classification Report: \\n')\n",
    "        print(classification_report(y_test,y_pred))\n",
    "    \n",
    "        print('Metrics:')\n",
    "        print(metrics_dataframe)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f90e11",
   "metadata": {},
   "source": [
    "## Quantum Kernel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565c66",
   "metadata": {},
   "source": [
    "It is also possible to train a quantum kernel with Quantum Kernel Alignment (QKA) that iteratively adapts a parametrized quantum kernel to a dataset and converging to the maximum SVM margin at the same time. To implement it, we prepare the dataset as usual and define the quantum feature map. Then, we will use QuantumKernelTrained.fit method to train the kernel parameters and pass it to a machine learning model.\n",
    "Source: https://lab.quantum-computing.ibm.com/user/5ae8692a0f0205003930696d/lab/workspaces/auto-s/tree/qiskit-tutorials/qiskit-machine-learning/08_quantum_kernel_trainer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b231d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading your IBM Quantum account(s)\n",
    "#from qiskit import IBMQ\n",
    "\n",
    "# Import utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn imports\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn import metrics\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "# Importing standard Qiskit libraries and Qiskit Machine Learning imports\n",
    "#from qiskit import BasicAer\n",
    "#from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "#from qiskit import QuantumCircuit\n",
    "#from qiskit.circuit import ParameterVector\n",
    "#from qiskit.providers.aer import AerSimulator\n",
    "#from qiskit.visualization import circuit_drawer\n",
    "#from qiskit.algorithms.optimizers import SPSA\n",
    "#from qiskit.circuit.library import ZZFeatureMap\n",
    "#from qiskit_machine_learning.kernels import QuantumKernel\n",
    "#from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer\n",
    "#from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "# Import dataset\n",
    "data = '../data/datasets/neurons.csv'\n",
    "neuron = pd.read_csv(data, delimiter=';')\n",
    "\n",
    "# Select a subset of the data composed of three classes\n",
    "df = neuron.head(20).copy()                    # Ganglion\n",
    "df = pd.concat([df, neuron.iloc[373:393]])     # Granule\n",
    "df = pd.concat([df, neuron.iloc[17033:17053]]) # Basket\n",
    "\n",
    "# seed for randomization, to keep outputs consistent\n",
    "from qiskit.utils import algorithm_globals\n",
    "seed = 123456\n",
    "algorithm_globals.random_seed = seed\n",
    "\n",
    "# Drop row having at least 1 missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating an instance of Labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "# Assigning numerical value and storing it\n",
    "df[[\"Target\"]] = df[[\"Target\"]].apply(enc.fit_transform)\n",
    "\n",
    "# Divide the data, y the variable to predict (Target) and X the features\n",
    "X = df[df.columns[1:]]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data : training and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Scaling the data\n",
    "from sklearn import preprocessing\n",
    "Normalize = preprocessing.StandardScaler()\n",
    "# Transform data\n",
    "X_train = Normalize.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "X_test = Normalize.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = X.columns)\n",
    "\n",
    "# Dimension Reduction with PCA (with two principal compoenents)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "# transform data\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "\n",
    "# Define a new DataFrame with two column (the principal components)\n",
    "component_columns = []\n",
    "for x in (n+1 for n in range(2)):\n",
    "    component_columns = component_columns + ['PCA_%i'%x]\n",
    "X_train = pd.DataFrame(data = X_train, columns = component_columns)\n",
    "X_test = pd.DataFrame(data = X_test, columns = component_columns)\n",
    "\n",
    "# Define some parameters\n",
    "feature_dimension = 2 # number of features\n",
    "reps = 2 # number of repetitions\n",
    "quantum_backend = 'ibmq_qasm_simulator' # quantum backend\n",
    "cv = 2 # Cross-validation\n",
    "circuits = 2 # number of circuits\n",
    "\n",
    "# The use of these requires us to sign with an IBMQ account.\n",
    "# Assuming the credentials are already loaded onto your computer, you sign in with\n",
    "#IBMQ.save_account('YOUR API')\n",
    "#IBMQ.load_account()\n",
    "#provider = IBMQ.get_provider(hub='ibm-q')\n",
    "\n",
    "# Define a callback class for our optimizer\n",
    "class QKTCallback:\n",
    "    \"\"\"Callback wrapper class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._data = [[] for i in range(5)]\n",
    "\n",
    "    def callback(self, x0, x1=None, x2=None, x3=None, x4=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x0: number of function evaluations\n",
    "            x1: the parameters\n",
    "            x2: the function value\n",
    "            x3: the stepsize\n",
    "            x4: whether the step was accepted\n",
    "        \"\"\"\n",
    "        self._data[0].append(x0)\n",
    "        self._data[1].append(x1)\n",
    "        self._data[2].append(x2)\n",
    "        self._data[3].append(x3)\n",
    "        self._data[4].append(x4)\n",
    "\n",
    "    def get_callback_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def clear_callback_data(self):\n",
    "        self._data = [[] for i in range(5)]\n",
    "    \n",
    "# seed for randomization, to keep outputs consistent\n",
    "#seed = 123456\n",
    "#algorithm_globals.random_seed = seed\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.visualization import circuit_drawer\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.kernels import TrainableFidelityQuantumKernel\n",
    "from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer\n",
    "        \n",
    "# Create a rotational layer to train. We will rotate each qubit the same amount.\n",
    "training_params = ParameterVector(\"θ\", 1)\n",
    "fm0 = QuantumCircuit(feature_dimension)\n",
    "for qubit in range(feature_dimension):\n",
    "    fm0.ry(training_params[0], qubit)\n",
    "        \n",
    "#fm0.ry(training_params[0], 0)\n",
    "#fm0.ry(training_params[0], 1)\n",
    "\n",
    "# Use ZZFeatureMap to represent input data\n",
    "fm1 = ZZFeatureMap(feature_dimension=feature_dimension, reps=reps, entanglement='linear')\n",
    "\n",
    "# Create the feature map, composed of our two circuits\n",
    "fm = fm0.compose(fm1)\n",
    "\n",
    "print(circuit_drawer(fm))\n",
    "print(f\"Trainable parameters: {training_params}\")\n",
    "    \n",
    "\n",
    "# Rotational layer to train. We rotate each qubit the same amount.\n",
    "#from qiskit.circuit import ParameterVector\n",
    "#user_params = ParameterVector(\"θ\", 1)\n",
    "#fm0 = QuantumCircuit(circuits) # Number of circuits\n",
    "#fm0.ry(user_params[0], 0)\n",
    "#fm0.ry(user_params[0], 1)\n",
    "\n",
    "# Use ZZFeatureMap \n",
    "#qfm_zz = ZZFeatureMap(feature_dimension)\n",
    "\n",
    "# Create the feature map\n",
    "#fm = fm0.compose(qfm_zz)\n",
    "\n",
    "#print(circuit_drawer(fm))\n",
    "#print(f\"Trainable parameters: {user_params}\")\n",
    "\n",
    "#print(qfm_zz)\n",
    "\n",
    "if quantum_backend is not None:\n",
    "    # Compute code with online quantum simulators or quantum hardware from the cloud\n",
    "    # Import QiskitRuntimeService and Sampler\n",
    "    from qiskit_ibm_runtime import QiskitRuntimeService, Sampler\n",
    "    # Define service\n",
    "    service = QiskitRuntimeService(channel = 'ibm_quantum', token = ibm_account, instance = 'ibm-q-internal/deployed/default')\n",
    "    # Get backend\n",
    "    backend = service.backend(quantum_backend) # Use a simulator or hardware from the cloud\n",
    "    # Define Sampler: With our training and testing datasets ready, we set up the FidelityQuantumKernel class to calculate a kernel matrix using the ZZFeatureMap. We use the reference implementation of the Sampler primitive and the ComputeUncompute fidelity that computes overlaps between states. These are the default values and if you don't pass a Sampler or Fidelity instance, the same objects will be created automatically for you.\n",
    "    # Run Quasi-Probability calculation\n",
    "    # optimization_level=3 adds dynamical decoupling\n",
    "    # resilience_level=1 adds readout error mitigation\n",
    "    from qiskit_ibm_runtime import Options\n",
    "    options = Options()\n",
    "    options.resilience_level = 1\n",
    "    options.execution.shots = 1\n",
    "    options.optimization_level = 3\n",
    "    sampler = Sampler(session=backend, options = options)\n",
    "else:\n",
    "    # Compute code with local simulator (Aer simulators)\n",
    "    from qiskit.primitives import Sampler\n",
    "    sampler = Sampler()\n",
    "\n",
    "# We utilize the default implementation of the Sampler primitive and the ComputeUncompute fidelity, which calculates the overlaps between states.\n",
    "# If you do not provide specific instances of Sampler or Fidelity, the code will automatically create these objects with the default values.\n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "    \n",
    "# Instantiate quantum kernel\n",
    "quant_kernel = TrainableFidelityQuantumKernel(fidelity = fidelity, feature_map=fm, training_parameters=training_params)\n",
    "\n",
    "# Set up the optimizer\n",
    "cb_qkt = QKTCallback()\n",
    "spsa_opt = SPSA(maxiter=10, callback=cb_qkt.callback, learning_rate=0.05, perturbation=0.05)\n",
    "\n",
    "# Instantiate a quantum kernel trainer\n",
    "qkt = QuantumKernelTrainer(\n",
    "    quantum_kernel=quant_kernel, loss=\"svc_loss\", optimizer=spsa_opt, initial_point=[np.pi / 2]\n",
    ")\n",
    "\n",
    "# Train the kernel using QKT directly\n",
    "qka_results = qkt.fit(X_train, y_train)\n",
    "optimized_kernel = qka_results.quantum_kernel\n",
    "print(qka_results)\n",
    "\n",
    "# Use QSVC for classification\n",
    "qsvc = QSVC(quantum_kernel=optimized_kernel)\n",
    "\n",
    "# Fit the QSVC\n",
    "qsvc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = qsvc.predict(X_test)\n",
    "\n",
    "# Evalaute the test accuracy\n",
    "accuracy_test = metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f\"accuracy test: {accuracy_test}\")\n",
    "\n",
    "# Print predicted values and real values of the X_test dataset\n",
    "print(\"\\n\")\n",
    "print(\"Print predicted data coming from X_test as new input data\")\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Print real values\\n\")\n",
    "print(y_test)\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Print accuracy metrics of the model\n",
    "results = [metrics.accuracy_score(y_test, y_pred),metrics.precision_score(y_test, y_pred, average='micro'),metrics.recall_score(y_test, y_pred, average='micro'),metrics.f1_score(y_test, y_pred, average='micro'), cross_val_score(qsvc, X_train, y_train, cv=cv).mean(), cross_val_score(qsvc, X_train, y_train, cv=cv).std()]\n",
    "metrics_dataframe = pd.DataFrame(results, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Cross-validation mean\", \"Cross-validation std\"], columns=['q_kernel_training'])\n",
    "print('Classification Report: \\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "        \n",
    "metrics_dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
